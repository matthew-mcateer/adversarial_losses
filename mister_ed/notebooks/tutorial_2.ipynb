{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training with `mister_ed`\n",
    "This file will contain the basics on how to perform adversarial training under the `mister_ed` framework. It's highly recommended that you have walked through tutorial_1 before going through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start by importing everything we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTERNAL LIBRARY IMPORTS\n",
    "import numpy as np \n",
    "import scipy \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch # Need torch version >=0.3\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "assert float(torch.__version__[:3]) >= 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISTER ED SPECIFIC IMPORT BLOCK\n",
    "# (here we do things so relative imports work )\n",
    "# Universal import block \n",
    "# Block to get the relative imports working \n",
    "import os\n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import config\n",
    "import prebuilt_loss_functions as plf\n",
    "import loss_functions as lf \n",
    "import utils.pytorch_utils as utils\n",
    "import utils.image_utils as img_utils\n",
    "import cifar10.cifar_loader as cifar_loader\n",
    "import cifar10.cifar_resnets as cifar_resnets\n",
    "import adversarial_training as advtrain\n",
    "import adversarial_evaluation as adveval\n",
    "import utils.checkpoints as checkpoints\n",
    "import adversarial_perturbations as ap \n",
    "import adversarial_attacks as aa\n",
    "import spatial_transformers as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define what we want to do here:\n",
    "\n",
    "Our goal is to run through a few training epochs of a pretrained classifier where we augment the training data with a set of adversarial examples. For simplicity's sake, let's just try and train a few epochs of a 20-layer ResNet trained on CIFAR-10, defended against an FGSM attack of $\\epsilon=8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's instatiate our pretrained classifier and our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_trainset = cifar_loader.load_cifar_data('train')\n",
    "model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's build the attack parameters: an object that contains all the information to perform an attack on a minibatch. So first let's build an attack object and then furnish it with the necessary kwargs. \n",
    "\n",
    "Like in tutorial 1, to create an attack object, we'll need to create a threat model and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(model, normalizer)\n",
    "attack_object = aa.FGSM(model, normalizer, delta_threat, attack_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we build the `AttackParameters` object, which just wraps the attack object with the kwargs needed to call the `attack(...)` method on attack. For FGSM attacks, we just want to turn the verbosity off, but for more complicated attacks, this will be more involved. Typically in training, we generate a single adversarial example per training point, but to be speedy here, let's only create 1 example per every 5 training points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our attack parameters built, we can build the object that handles training for us: this is instatiated with knowledge of the classifier, normalizer and some identifying features such as the *name* of the experiment and architecture. It's worthwhile to be informative with these so you keep which attacks this model is trained against straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'tutorial_fgsm'\n",
    "architecture = 'resnet20'\n",
    "training_obj = advtrain.AdversarialTraining(model, normalizer, experiment_name, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start training though, you'll need to furnish the trainer with some extra arguments:\n",
    "    - the data loader \n",
    "    - the number of epochs to train for \n",
    "    - a loss function (not one of the `mister_ed` custom loss functions though!)\n",
    "    - which optimizer to use (defaults to Adam with decent hyperparams)\n",
    "    - the attack parameters object \n",
    "    - whether or not to use the gpu (defaults to not using GPU)\n",
    "    - the verbosity level (ranging from ['low', 'medium', 'high', 'snoop'] (defaults to 'medium')\n",
    "    - whether or not to save the generated adversarial examples as images (defaults to false)\n",
    "    \n",
    "To be cute, we'll just train for two epochs so you get the picture. Also note that unless the verbosity is set to `low`, a checkpoint will be saved after every epoch. By default, these checkpoints are named like `<experiment_name>.<architecture_name>.<epoch>.path.tar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adv_attack_obj': <adversarial_attacks.FGSM at 0x7f035a3671d0>,\n",
       " 'attack_kwargs': {'verbose': False},\n",
       " 'attack_specific_params': {'attack_kwargs': {'verbose': False}},\n",
       " 'proportion_attacked': 0.2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_params.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['adv_attack_obj', 'proportion_attacked', 'attack_specific_params', 'attack_kwargs'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_params.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_DataLoader__initialized': True,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7f037bbe80f0>,\n",
       " 'batch_size': 128,\n",
       " 'collate_fn': <function torch.utils.data.dataloader.default_collate>,\n",
       " 'dataset': Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Split: train\n",
       "     Root Location: /home/mmcateer0/datasets\n",
       "     Transforms (if any): Compose(\n",
       "                              RandomHorizontalFlip(p=0.5)\n",
       "                              RandomCrop(size=(32, 32), padding=4)\n",
       "                              ToTensor()\n",
       "                          )\n",
       "     Target Transforms (if any): None,\n",
       " 'drop_last': False,\n",
       " 'num_workers': 4,\n",
       " 'pin_memory': True,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7f037bbe82b0>,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_trainset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['classifier_net', 'normalizer', 'experiment_name', 'architecture_name', 'use_gpu', 'verbosity_level', 'verbosity_minibatch', 'verbosity_adv', 'verbosity_epoch', 'logger', 'log_level', 'log_minibatch', 'log_adv', 'log_epoch'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_obj.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] accuracy: (12.000, 100.000)\n",
      "[1,     1] loss: 1.397052\n",
      "[1,     2] accuracy: (20.000, 100.000)\n",
      "[1,     2] loss: 1.152927\n",
      "[1,     3] accuracy: (24.000, 100.000)\n",
      "[1,     3] loss: 1.175319\n",
      "[1,     4] accuracy: (20.000, 100.000)\n",
      "[1,     4] loss: 0.941684\n",
      "[1,     5] accuracy: (8.000, 96.000)\n",
      "[1,     5] loss: 1.140610\n",
      "[1,     6] accuracy: (8.000, 96.000)\n",
      "[1,     6] loss: 1.090311\n",
      "[1,     7] accuracy: (16.000, 96.000)\n",
      "[1,     7] loss: 1.115436\n",
      "[1,     8] accuracy: (20.000, 100.000)\n",
      "[1,     8] loss: 0.774957\n",
      "[1,     9] accuracy: (24.000, 92.000)\n",
      "[1,     9] loss: 0.807581\n",
      "[1,    10] accuracy: (16.000, 88.000)\n",
      "[1,    10] loss: 0.847496\n",
      "[1,    11] accuracy: (20.000, 92.000)\n",
      "[1,    11] loss: 0.550180\n",
      "[1,    12] accuracy: (20.000, 96.000)\n",
      "[1,    12] loss: 0.585428\n",
      "[1,    13] accuracy: (20.000, 88.000)\n",
      "[1,    13] loss: 0.555863\n",
      "[1,    14] accuracy: (16.000, 80.000)\n",
      "[1,    14] loss: 0.521924\n",
      "[1,    15] accuracy: (32.000, 88.000)\n",
      "[1,    15] loss: 0.494853\n",
      "[1,    16] accuracy: (24.000, 92.000)\n",
      "[1,    16] loss: 0.485020\n",
      "[1,    17] accuracy: (12.000, 88.000)\n",
      "[1,    17] loss: 0.530921\n",
      "[1,    18] accuracy: (24.000, 88.000)\n",
      "[1,    18] loss: 0.541836\n",
      "[1,    19] accuracy: (32.000, 96.000)\n",
      "[1,    19] loss: 0.415283\n",
      "[1,    20] accuracy: (16.000, 92.000)\n",
      "[1,    20] loss: 0.499473\n",
      "[1,    21] accuracy: (24.000, 84.000)\n",
      "[1,    21] loss: 0.622731\n",
      "[1,    22] accuracy: (12.000, 92.000)\n",
      "[1,    22] loss: 0.569643\n",
      "[1,    23] accuracy: (12.000, 88.000)\n",
      "[1,    23] loss: 0.510514\n",
      "[1,    24] accuracy: (16.000, 88.000)\n",
      "[1,    24] loss: 0.517392\n",
      "[1,    25] accuracy: (12.000, 72.000)\n",
      "[1,    25] loss: 0.539248\n",
      "[1,    26] accuracy: (16.000, 92.000)\n",
      "[1,    26] loss: 0.505164\n",
      "[1,    27] accuracy: (8.000, 96.000)\n",
      "[1,    27] loss: 0.583992\n",
      "[1,    28] accuracy: (24.000, 88.000)\n",
      "[1,    28] loss: 0.501339\n",
      "[1,    29] accuracy: (32.000, 92.000)\n",
      "[1,    29] loss: 0.471640\n",
      "[1,    30] accuracy: (32.000, 92.000)\n",
      "[1,    30] loss: 0.439169\n",
      "[1,    31] accuracy: (24.000, 96.000)\n",
      "[1,    31] loss: 0.627135\n",
      "[1,    32] accuracy: (16.000, 84.000)\n",
      "[1,    32] loss: 0.455460\n",
      "[1,    33] accuracy: (44.000, 88.000)\n",
      "[1,    33] loss: 0.414642\n",
      "[1,    34] accuracy: (36.000, 92.000)\n",
      "[1,    34] loss: 0.472207\n",
      "[1,    35] accuracy: (32.000, 96.000)\n",
      "[1,    35] loss: 0.375050\n",
      "[1,    36] accuracy: (24.000, 84.000)\n",
      "[1,    36] loss: 0.455720\n",
      "[1,    37] accuracy: (24.000, 92.000)\n",
      "[1,    37] loss: 0.494296\n",
      "[1,    38] accuracy: (20.000, 88.000)\n",
      "[1,    38] loss: 0.507213\n",
      "[1,    39] accuracy: (16.000, 92.000)\n",
      "[1,    39] loss: 0.446010\n",
      "[1,    40] accuracy: (16.000, 84.000)\n",
      "[1,    40] loss: 0.456658\n",
      "[1,    41] accuracy: (12.000, 92.000)\n",
      "[1,    41] loss: 0.410436\n",
      "[1,    42] accuracy: (40.000, 100.000)\n",
      "[1,    42] loss: 0.370548\n",
      "[1,    43] accuracy: (28.000, 96.000)\n",
      "[1,    43] loss: 0.417388\n",
      "[1,    44] accuracy: (32.000, 88.000)\n",
      "[1,    44] loss: 0.459322\n",
      "[1,    45] accuracy: (20.000, 88.000)\n",
      "[1,    45] loss: 0.545190\n",
      "[1,    46] accuracy: (16.000, 100.000)\n",
      "[1,    46] loss: 0.437088\n",
      "[1,    47] accuracy: (24.000, 84.000)\n",
      "[1,    47] loss: 0.492457\n",
      "[1,    48] accuracy: (12.000, 84.000)\n",
      "[1,    48] loss: 0.490094\n",
      "[1,    49] accuracy: (32.000, 92.000)\n",
      "[1,    49] loss: 0.372153\n",
      "[1,    50] accuracy: (36.000, 92.000)\n",
      "[1,    50] loss: 0.443639\n",
      "[1,    51] accuracy: (24.000, 100.000)\n",
      "[1,    51] loss: 0.423852\n",
      "[1,    52] accuracy: (36.000, 96.000)\n",
      "[1,    52] loss: 0.363795\n",
      "[1,    53] accuracy: (40.000, 96.000)\n",
      "[1,    53] loss: 0.423629\n",
      "[1,    54] accuracy: (36.000, 96.000)\n",
      "[1,    54] loss: 0.413175\n",
      "[1,    55] accuracy: (24.000, 92.000)\n",
      "[1,    55] loss: 0.390121\n",
      "[1,    56] accuracy: (16.000, 92.000)\n",
      "[1,    56] loss: 0.584133\n",
      "[1,    57] accuracy: (28.000, 92.000)\n",
      "[1,    57] loss: 0.417106\n",
      "[1,    58] accuracy: (44.000, 96.000)\n",
      "[1,    58] loss: 0.397308\n",
      "[1,    59] accuracy: (44.000, 96.000)\n",
      "[1,    59] loss: 0.292709\n",
      "[1,    60] accuracy: (40.000, 88.000)\n",
      "[1,    60] loss: 0.366099\n",
      "[1,    61] accuracy: (16.000, 92.000)\n",
      "[1,    61] loss: 0.357684\n",
      "[1,    62] accuracy: (44.000, 100.000)\n",
      "[1,    62] loss: 0.350061\n",
      "[1,    63] accuracy: (32.000, 100.000)\n",
      "[1,    63] loss: 0.435909\n",
      "[1,    64] accuracy: (24.000, 96.000)\n",
      "[1,    64] loss: 0.356239\n",
      "[1,    65] accuracy: (36.000, 84.000)\n",
      "[1,    65] loss: 0.421362\n",
      "[1,    66] accuracy: (36.000, 96.000)\n",
      "[1,    66] loss: 0.417380\n",
      "[1,    67] accuracy: (28.000, 100.000)\n",
      "[1,    67] loss: 0.420230\n",
      "[1,    68] accuracy: (36.000, 96.000)\n",
      "[1,    68] loss: 0.489867\n",
      "[1,    69] accuracy: (40.000, 96.000)\n",
      "[1,    69] loss: 0.350028\n",
      "[1,    70] accuracy: (16.000, 96.000)\n",
      "[1,    70] loss: 0.537107\n",
      "[1,    71] accuracy: (24.000, 96.000)\n",
      "[1,    71] loss: 0.432984\n",
      "[1,    72] accuracy: (28.000, 100.000)\n",
      "[1,    72] loss: 0.405976\n",
      "[1,    73] accuracy: (28.000, 100.000)\n",
      "[1,    73] loss: 0.329066\n",
      "[1,    74] accuracy: (44.000, 92.000)\n",
      "[1,    74] loss: 0.331164\n",
      "[1,    75] accuracy: (20.000, 96.000)\n",
      "[1,    75] loss: 0.390398\n",
      "[1,    76] accuracy: (36.000, 100.000)\n",
      "[1,    76] loss: 0.378705\n",
      "[1,    77] accuracy: (20.000, 92.000)\n",
      "[1,    77] loss: 0.498878\n",
      "[1,    78] accuracy: (24.000, 100.000)\n",
      "[1,    78] loss: 0.347126\n",
      "[1,    79] accuracy: (20.000, 100.000)\n",
      "[1,    79] loss: 0.407233\n",
      "[1,    80] accuracy: (44.000, 92.000)\n",
      "[1,    80] loss: 0.350687\n",
      "[1,    81] accuracy: (28.000, 92.000)\n",
      "[1,    81] loss: 0.344436\n",
      "[1,    82] accuracy: (32.000, 92.000)\n",
      "[1,    82] loss: 0.415996\n",
      "[1,    83] accuracy: (36.000, 92.000)\n",
      "[1,    83] loss: 0.347523\n",
      "[1,    84] accuracy: (48.000, 96.000)\n",
      "[1,    84] loss: 0.312565\n",
      "[1,    85] accuracy: (36.000, 88.000)\n",
      "[1,    85] loss: 0.476292\n",
      "[1,    86] accuracy: (28.000, 96.000)\n",
      "[1,    86] loss: 0.355719\n",
      "[1,    87] accuracy: (44.000, 100.000)\n",
      "[1,    87] loss: 0.341995\n",
      "[1,    88] accuracy: (32.000, 100.000)\n",
      "[1,    88] loss: 0.380763\n",
      "[1,    89] accuracy: (36.000, 100.000)\n",
      "[1,    89] loss: 0.351690\n",
      "[1,    90] accuracy: (36.000, 100.000)\n",
      "[1,    90] loss: 0.451751\n",
      "[1,    91] accuracy: (44.000, 100.000)\n",
      "[1,    91] loss: 0.355467\n",
      "[1,    92] accuracy: (36.000, 96.000)\n",
      "[1,    92] loss: 0.322781\n",
      "[1,    93] accuracy: (8.000, 100.000)\n",
      "[1,    93] loss: 0.434486\n",
      "[1,    94] accuracy: (32.000, 100.000)\n",
      "[1,    94] loss: 0.409912\n",
      "[1,    95] accuracy: (28.000, 92.000)\n",
      "[1,    95] loss: 0.440459\n",
      "[1,    96] accuracy: (40.000, 100.000)\n",
      "[1,    96] loss: 0.305283\n",
      "[1,    97] accuracy: (36.000, 100.000)\n",
      "[1,    97] loss: 0.412027\n",
      "[1,    98] accuracy: (20.000, 92.000)\n",
      "[1,    98] loss: 0.350911\n",
      "[1,    99] accuracy: (24.000, 96.000)\n",
      "[1,    99] loss: 0.398615\n",
      "[1,   100] accuracy: (32.000, 96.000)\n",
      "[1,   100] loss: 0.388585\n",
      "[1,   101] accuracy: (36.000, 96.000)\n",
      "[1,   101] loss: 0.408542\n",
      "[1,   102] accuracy: (36.000, 100.000)\n",
      "[1,   102] loss: 0.425794\n",
      "[1,   103] accuracy: (28.000, 92.000)\n",
      "[1,   103] loss: 0.433206\n",
      "[1,   104] accuracy: (20.000, 92.000)\n",
      "[1,   104] loss: 0.383348\n",
      "[1,   105] accuracy: (36.000, 100.000)\n",
      "[1,   105] loss: 0.345400\n",
      "[1,   106] accuracy: (24.000, 96.000)\n",
      "[1,   106] loss: 0.380458\n",
      "[1,   107] accuracy: (24.000, 100.000)\n",
      "[1,   107] loss: 0.491432\n",
      "[1,   108] accuracy: (40.000, 88.000)\n",
      "[1,   108] loss: 0.443756\n",
      "[1,   109] accuracy: (36.000, 92.000)\n",
      "[1,   109] loss: 0.384557\n",
      "[1,   110] accuracy: (36.000, 92.000)\n",
      "[1,   110] loss: 0.387167\n",
      "[1,   111] accuracy: (40.000, 100.000)\n",
      "[1,   111] loss: 0.329424\n",
      "[1,   112] accuracy: (24.000, 92.000)\n",
      "[1,   112] loss: 0.410733\n",
      "[1,   113] accuracy: (40.000, 92.000)\n",
      "[1,   113] loss: 0.326469\n",
      "[1,   114] accuracy: (28.000, 92.000)\n",
      "[1,   114] loss: 0.373526\n",
      "[1,   115] accuracy: (32.000, 96.000)\n",
      "[1,   115] loss: 0.382943\n",
      "[1,   116] accuracy: (52.000, 96.000)\n",
      "[1,   116] loss: 0.363083\n",
      "[1,   117] accuracy: (32.000, 92.000)\n",
      "[1,   117] loss: 0.350046\n",
      "[1,   118] accuracy: (36.000, 96.000)\n",
      "[1,   118] loss: 0.416980\n",
      "[1,   119] accuracy: (36.000, 96.000)\n",
      "[1,   119] loss: 0.489425\n",
      "[1,   120] accuracy: (32.000, 100.000)\n",
      "[1,   120] loss: 0.367913\n",
      "[1,   121] accuracy: (28.000, 100.000)\n",
      "[1,   121] loss: 0.371968\n",
      "[1,   122] accuracy: (20.000, 100.000)\n",
      "[1,   122] loss: 0.404247\n",
      "[1,   123] accuracy: (24.000, 100.000)\n",
      "[1,   123] loss: 0.402716\n",
      "[1,   124] accuracy: (24.000, 96.000)\n",
      "[1,   124] loss: 0.403214\n",
      "[1,   125] accuracy: (40.000, 92.000)\n",
      "[1,   125] loss: 0.372752\n",
      "[1,   126] accuracy: (28.000, 88.000)\n",
      "[1,   126] loss: 0.375417\n",
      "[1,   127] accuracy: (52.000, 100.000)\n",
      "[1,   127] loss: 0.319035\n",
      "[1,   128] accuracy: (32.000, 96.000)\n",
      "[1,   128] loss: 0.383265\n",
      "[1,   129] accuracy: (40.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   129] loss: 0.385624\n",
      "[1,   130] accuracy: (20.000, 100.000)\n",
      "[1,   130] loss: 0.345048\n",
      "[1,   131] accuracy: (52.000, 100.000)\n",
      "[1,   131] loss: 0.320810\n",
      "[1,   132] accuracy: (40.000, 88.000)\n",
      "[1,   132] loss: 0.341209\n",
      "[1,   133] accuracy: (40.000, 96.000)\n",
      "[1,   133] loss: 0.310713\n",
      "[1,   134] accuracy: (32.000, 100.000)\n",
      "[1,   134] loss: 0.428583\n",
      "[1,   135] accuracy: (44.000, 92.000)\n",
      "[1,   135] loss: 0.415571\n",
      "[1,   136] accuracy: (40.000, 84.000)\n",
      "[1,   136] loss: 0.430168\n",
      "[1,   137] accuracy: (52.000, 92.000)\n",
      "[1,   137] loss: 0.399173\n",
      "[1,   138] accuracy: (44.000, 96.000)\n",
      "[1,   138] loss: 0.410206\n",
      "[1,   139] accuracy: (24.000, 96.000)\n",
      "[1,   139] loss: 0.355550\n",
      "[1,   140] accuracy: (40.000, 100.000)\n",
      "[1,   140] loss: 0.385065\n",
      "[1,   141] accuracy: (36.000, 96.000)\n",
      "[1,   141] loss: 0.357291\n",
      "[1,   142] accuracy: (20.000, 100.000)\n",
      "[1,   142] loss: 0.355178\n",
      "[1,   143] accuracy: (28.000, 96.000)\n",
      "[1,   143] loss: 0.353244\n",
      "[1,   144] accuracy: (32.000, 96.000)\n",
      "[1,   144] loss: 0.342416\n",
      "[1,   145] accuracy: (36.000, 92.000)\n",
      "[1,   145] loss: 0.442396\n",
      "[1,   146] accuracy: (32.000, 100.000)\n",
      "[1,   146] loss: 0.370204\n",
      "[1,   147] accuracy: (32.000, 92.000)\n",
      "[1,   147] loss: 0.314353\n",
      "[1,   148] accuracy: (36.000, 100.000)\n",
      "[1,   148] loss: 0.314144\n",
      "[1,   149] accuracy: (48.000, 100.000)\n",
      "[1,   149] loss: 0.394002\n",
      "[1,   150] accuracy: (28.000, 92.000)\n",
      "[1,   150] loss: 0.325966\n",
      "[1,   151] accuracy: (48.000, 92.000)\n",
      "[1,   151] loss: 0.359490\n",
      "[1,   152] accuracy: (28.000, 100.000)\n",
      "[1,   152] loss: 0.371073\n",
      "[1,   153] accuracy: (44.000, 96.000)\n",
      "[1,   153] loss: 0.353006\n",
      "[1,   154] accuracy: (48.000, 100.000)\n",
      "[1,   154] loss: 0.336850\n",
      "[1,   155] accuracy: (28.000, 96.000)\n",
      "[1,   155] loss: 0.383638\n",
      "[1,   156] accuracy: (36.000, 92.000)\n",
      "[1,   156] loss: 0.393299\n",
      "[1,   157] accuracy: (52.000, 96.000)\n",
      "[1,   157] loss: 0.292750\n",
      "[1,   158] accuracy: (40.000, 96.000)\n",
      "[1,   158] loss: 0.393195\n",
      "[1,   159] accuracy: (52.000, 96.000)\n",
      "[1,   159] loss: 0.336942\n",
      "[1,   160] accuracy: (40.000, 96.000)\n",
      "[1,   160] loss: 0.325113\n",
      "[1,   161] accuracy: (24.000, 92.000)\n",
      "[1,   161] loss: 0.379535\n",
      "[1,   162] accuracy: (40.000, 92.000)\n",
      "[1,   162] loss: 0.358963\n",
      "[1,   163] accuracy: (28.000, 100.000)\n",
      "[1,   163] loss: 0.352377\n",
      "[1,   164] accuracy: (36.000, 100.000)\n",
      "[1,   164] loss: 0.339393\n",
      "[1,   165] accuracy: (44.000, 100.000)\n",
      "[1,   165] loss: 0.323073\n",
      "[1,   166] accuracy: (36.000, 100.000)\n",
      "[1,   166] loss: 0.315088\n",
      "[1,   167] accuracy: (28.000, 96.000)\n",
      "[1,   167] loss: 0.376075\n",
      "[1,   168] accuracy: (32.000, 84.000)\n",
      "[1,   168] loss: 0.478747\n",
      "[1,   169] accuracy: (20.000, 96.000)\n",
      "[1,   169] loss: 0.449689\n",
      "[1,   170] accuracy: (28.000, 96.000)\n",
      "[1,   170] loss: 0.313008\n",
      "[1,   171] accuracy: (52.000, 100.000)\n",
      "[1,   171] loss: 0.296134\n",
      "[1,   172] accuracy: (20.000, 100.000)\n",
      "[1,   172] loss: 0.461419\n",
      "[1,   173] accuracy: (36.000, 96.000)\n",
      "[1,   173] loss: 0.336432\n",
      "[1,   174] accuracy: (40.000, 96.000)\n",
      "[1,   174] loss: 0.366429\n",
      "[1,   175] accuracy: (56.000, 96.000)\n",
      "[1,   175] loss: 0.270344\n",
      "[1,   176] accuracy: (16.000, 96.000)\n",
      "[1,   176] loss: 0.588556\n",
      "[1,   177] accuracy: (32.000, 92.000)\n",
      "[1,   177] loss: 0.351444\n",
      "[1,   178] accuracy: (64.000, 96.000)\n",
      "[1,   178] loss: 0.318880\n",
      "[1,   179] accuracy: (32.000, 100.000)\n",
      "[1,   179] loss: 0.366064\n",
      "[1,   180] accuracy: (32.000, 100.000)\n",
      "[1,   180] loss: 0.278974\n",
      "[1,   181] accuracy: (48.000, 100.000)\n",
      "[1,   181] loss: 0.331230\n",
      "[1,   182] accuracy: (48.000, 100.000)\n",
      "[1,   182] loss: 0.386196\n",
      "[1,   183] accuracy: (40.000, 96.000)\n",
      "[1,   183] loss: 0.330566\n",
      "[1,   184] accuracy: (28.000, 96.000)\n",
      "[1,   184] loss: 0.364085\n",
      "[1,   185] accuracy: (44.000, 100.000)\n",
      "[1,   185] loss: 0.298717\n",
      "[1,   186] accuracy: (48.000, 96.000)\n",
      "[1,   186] loss: 0.335234\n",
      "[1,   187] accuracy: (32.000, 100.000)\n",
      "[1,   187] loss: 0.352552\n",
      "[1,   188] accuracy: (32.000, 100.000)\n",
      "[1,   188] loss: 0.324726\n",
      "[1,   189] accuracy: (52.000, 96.000)\n",
      "[1,   189] loss: 0.352205\n",
      "[1,   190] accuracy: (20.000, 88.000)\n",
      "[1,   190] loss: 0.436278\n",
      "[1,   191] accuracy: (40.000, 100.000)\n",
      "[1,   191] loss: 0.298245\n",
      "[1,   192] accuracy: (24.000, 92.000)\n",
      "[1,   192] loss: 0.330231\n",
      "[1,   193] accuracy: (44.000, 100.000)\n",
      "[1,   193] loss: 0.359893\n",
      "[1,   194] accuracy: (32.000, 92.000)\n",
      "[1,   194] loss: 0.396701\n",
      "[1,   195] accuracy: (40.000, 100.000)\n",
      "[1,   195] loss: 0.317055\n",
      "[1,   196] accuracy: (40.000, 100.000)\n",
      "[1,   196] loss: 0.305350\n",
      "[1,   197] accuracy: (40.000, 92.000)\n",
      "[1,   197] loss: 0.397312\n",
      "[1,   198] accuracy: (60.000, 100.000)\n",
      "[1,   198] loss: 0.247826\n",
      "[1,   199] accuracy: (36.000, 100.000)\n",
      "[1,   199] loss: 0.349587\n",
      "[1,   200] accuracy: (44.000, 92.000)\n",
      "[1,   200] loss: 0.334864\n",
      "[1,   201] accuracy: (36.000, 96.000)\n",
      "[1,   201] loss: 0.383428\n",
      "[1,   202] accuracy: (32.000, 96.000)\n",
      "[1,   202] loss: 0.308984\n",
      "[1,   203] accuracy: (48.000, 100.000)\n",
      "[1,   203] loss: 0.254954\n",
      "[1,   204] accuracy: (36.000, 96.000)\n",
      "[1,   204] loss: 0.354388\n",
      "[1,   205] accuracy: (52.000, 92.000)\n",
      "[1,   205] loss: 0.375867\n",
      "[1,   206] accuracy: (24.000, 96.000)\n",
      "[1,   206] loss: 0.394872\n",
      "[1,   207] accuracy: (36.000, 100.000)\n",
      "[1,   207] loss: 0.432942\n",
      "[1,   208] accuracy: (32.000, 92.000)\n",
      "[1,   208] loss: 0.401338\n",
      "[1,   209] accuracy: (24.000, 96.000)\n",
      "[1,   209] loss: 0.375708\n",
      "[1,   210] accuracy: (48.000, 96.000)\n",
      "[1,   210] loss: 0.302584\n",
      "[1,   211] accuracy: (44.000, 92.000)\n",
      "[1,   211] loss: 0.383215\n",
      "[1,   212] accuracy: (32.000, 96.000)\n",
      "[1,   212] loss: 0.332046\n",
      "[1,   213] accuracy: (28.000, 100.000)\n",
      "[1,   213] loss: 0.349768\n",
      "[1,   214] accuracy: (32.000, 100.000)\n",
      "[1,   214] loss: 0.345506\n",
      "[1,   215] accuracy: (44.000, 100.000)\n",
      "[1,   215] loss: 0.334799\n",
      "[1,   216] accuracy: (48.000, 96.000)\n",
      "[1,   216] loss: 0.331099\n",
      "[1,   217] accuracy: (28.000, 88.000)\n",
      "[1,   217] loss: 0.375643\n",
      "[1,   218] accuracy: (32.000, 96.000)\n",
      "[1,   218] loss: 0.424422\n",
      "[1,   219] accuracy: (52.000, 100.000)\n",
      "[1,   219] loss: 0.362258\n",
      "[1,   220] accuracy: (32.000, 92.000)\n",
      "[1,   220] loss: 0.422277\n",
      "[1,   221] accuracy: (32.000, 96.000)\n",
      "[1,   221] loss: 0.304690\n",
      "[1,   222] accuracy: (48.000, 96.000)\n",
      "[1,   222] loss: 0.321758\n",
      "[1,   223] accuracy: (28.000, 84.000)\n",
      "[1,   223] loss: 0.394315\n",
      "[1,   224] accuracy: (36.000, 96.000)\n",
      "[1,   224] loss: 0.329762\n",
      "[1,   225] accuracy: (24.000, 100.000)\n",
      "[1,   225] loss: 0.405864\n",
      "[1,   226] accuracy: (40.000, 92.000)\n",
      "[1,   226] loss: 0.393584\n",
      "[1,   227] accuracy: (44.000, 96.000)\n",
      "[1,   227] loss: 0.412761\n",
      "[1,   228] accuracy: (40.000, 96.000)\n",
      "[1,   228] loss: 0.352472\n",
      "[1,   229] accuracy: (52.000, 92.000)\n",
      "[1,   229] loss: 0.348826\n",
      "[1,   230] accuracy: (52.000, 100.000)\n",
      "[1,   230] loss: 0.264339\n",
      "[1,   231] accuracy: (36.000, 96.000)\n",
      "[1,   231] loss: 0.361618\n",
      "[1,   232] accuracy: (20.000, 96.000)\n",
      "[1,   232] loss: 0.395425\n",
      "[1,   233] accuracy: (48.000, 96.000)\n",
      "[1,   233] loss: 0.310822\n",
      "[1,   234] accuracy: (24.000, 88.000)\n",
      "[1,   234] loss: 0.429747\n",
      "[1,   235] accuracy: (40.000, 96.000)\n",
      "[1,   235] loss: 0.400492\n",
      "[1,   236] accuracy: (40.000, 96.000)\n",
      "[1,   236] loss: 0.318774\n",
      "[1,   237] accuracy: (44.000, 100.000)\n",
      "[1,   237] loss: 0.426196\n",
      "[1,   238] accuracy: (36.000, 96.000)\n",
      "[1,   238] loss: 0.341772\n",
      "[1,   239] accuracy: (16.000, 96.000)\n",
      "[1,   239] loss: 0.392725\n",
      "[1,   240] accuracy: (52.000, 96.000)\n",
      "[1,   240] loss: 0.313971\n",
      "[1,   241] accuracy: (48.000, 100.000)\n",
      "[1,   241] loss: 0.290809\n",
      "[1,   242] accuracy: (52.000, 100.000)\n",
      "[1,   242] loss: 0.284865\n",
      "[1,   243] accuracy: (48.000, 100.000)\n",
      "[1,   243] loss: 0.257385\n",
      "[1,   244] accuracy: (48.000, 100.000)\n",
      "[1,   244] loss: 0.302355\n",
      "[1,   245] accuracy: (40.000, 96.000)\n",
      "[1,   245] loss: 0.311466\n",
      "[1,   246] accuracy: (24.000, 96.000)\n",
      "[1,   246] loss: 0.369643\n",
      "[1,   247] accuracy: (36.000, 96.000)\n",
      "[1,   247] loss: 0.340110\n",
      "[1,   248] accuracy: (48.000, 92.000)\n",
      "[1,   248] loss: 0.390861\n",
      "[1,   249] accuracy: (44.000, 100.000)\n",
      "[1,   249] loss: 0.389509\n",
      "[1,   250] accuracy: (32.000, 96.000)\n",
      "[1,   250] loss: 0.426288\n",
      "[1,   251] accuracy: (36.000, 96.000)\n",
      "[1,   251] loss: 0.333576\n",
      "[1,   252] accuracy: (40.000, 100.000)\n",
      "[1,   252] loss: 0.321437\n",
      "[1,   253] accuracy: (40.000, 100.000)\n",
      "[1,   253] loss: 0.272035\n",
      "[1,   254] accuracy: (40.000, 92.000)\n",
      "[1,   254] loss: 0.368431\n",
      "[1,   255] accuracy: (32.000, 96.000)\n",
      "[1,   255] loss: 0.364564\n",
      "[1,   256] accuracy: (36.000, 100.000)\n",
      "[1,   256] loss: 0.404167\n",
      "[1,   257] accuracy: (48.000, 100.000)\n",
      "[1,   257] loss: 0.353212\n",
      "[1,   258] accuracy: (24.000, 100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   258] loss: 0.343649\n",
      "[1,   259] accuracy: (20.000, 96.000)\n",
      "[1,   259] loss: 0.338943\n",
      "[1,   260] accuracy: (32.000, 100.000)\n",
      "[1,   260] loss: 0.270145\n",
      "[1,   261] accuracy: (36.000, 96.000)\n",
      "[1,   261] loss: 0.401885\n",
      "[1,   262] accuracy: (32.000, 96.000)\n",
      "[1,   262] loss: 0.381377\n",
      "[1,   263] accuracy: (44.000, 96.000)\n",
      "[1,   263] loss: 0.396679\n",
      "[1,   264] accuracy: (52.000, 100.000)\n",
      "[1,   264] loss: 0.376909\n",
      "[1,   265] accuracy: (44.000, 96.000)\n",
      "[1,   265] loss: 0.401168\n",
      "[1,   266] accuracy: (48.000, 96.000)\n",
      "[1,   266] loss: 0.416303\n",
      "[1,   267] accuracy: (32.000, 96.000)\n",
      "[1,   267] loss: 0.428052\n",
      "[1,   268] accuracy: (32.000, 96.000)\n",
      "[1,   268] loss: 0.340108\n",
      "[1,   269] accuracy: (44.000, 100.000)\n",
      "[1,   269] loss: 0.311334\n",
      "[1,   270] accuracy: (20.000, 92.000)\n",
      "[1,   270] loss: 0.391099\n",
      "[1,   271] accuracy: (48.000, 92.000)\n",
      "[1,   271] loss: 0.356080\n",
      "[1,   272] accuracy: (36.000, 96.000)\n",
      "[1,   272] loss: 0.379954\n",
      "[1,   273] accuracy: (32.000, 100.000)\n",
      "[1,   273] loss: 0.330372\n",
      "[1,   274] accuracy: (28.000, 92.000)\n",
      "[1,   274] loss: 0.439132\n",
      "[1,   275] accuracy: (36.000, 88.000)\n",
      "[1,   275] loss: 0.392824\n",
      "[1,   276] accuracy: (40.000, 92.000)\n",
      "[1,   276] loss: 0.366565\n",
      "[1,   277] accuracy: (36.000, 96.000)\n",
      "[1,   277] loss: 0.388704\n",
      "[1,   278] accuracy: (64.000, 96.000)\n",
      "[1,   278] loss: 0.315967\n",
      "[1,   279] accuracy: (40.000, 96.000)\n",
      "[1,   279] loss: 0.341148\n",
      "[1,   280] accuracy: (52.000, 96.000)\n",
      "[1,   280] loss: 0.286075\n",
      "[1,   281] accuracy: (40.000, 96.000)\n",
      "[1,   281] loss: 0.403213\n",
      "[1,   282] accuracy: (24.000, 84.000)\n",
      "[1,   282] loss: 0.459390\n",
      "[1,   283] accuracy: (44.000, 100.000)\n",
      "[1,   283] loss: 0.345410\n",
      "[1,   284] accuracy: (28.000, 96.000)\n",
      "[1,   284] loss: 0.317719\n",
      "[1,   285] accuracy: (32.000, 92.000)\n",
      "[1,   285] loss: 0.388743\n",
      "[1,   286] accuracy: (32.000, 92.000)\n",
      "[1,   286] loss: 0.394840\n",
      "[1,   287] accuracy: (36.000, 100.000)\n",
      "[1,   287] loss: 0.401738\n",
      "[1,   288] accuracy: (36.000, 88.000)\n",
      "[1,   288] loss: 0.385755\n",
      "[1,   289] accuracy: (44.000, 96.000)\n",
      "[1,   289] loss: 0.492701\n",
      "[1,   290] accuracy: (40.000, 100.000)\n",
      "[1,   290] loss: 0.330356\n",
      "[1,   291] accuracy: (36.000, 96.000)\n",
      "[1,   291] loss: 0.345929\n",
      "[1,   292] accuracy: (48.000, 92.000)\n",
      "[1,   292] loss: 0.400153\n",
      "[1,   293] accuracy: (36.000, 92.000)\n",
      "[1,   293] loss: 0.356407\n",
      "[1,   294] accuracy: (40.000, 92.000)\n",
      "[1,   294] loss: 0.301725\n",
      "[1,   295] accuracy: (48.000, 92.000)\n",
      "[1,   295] loss: 0.275362\n",
      "[1,   296] accuracy: (36.000, 96.000)\n",
      "[1,   296] loss: 0.368301\n",
      "[1,   297] accuracy: (40.000, 100.000)\n",
      "[1,   297] loss: 0.364455\n",
      "[1,   298] accuracy: (36.000, 100.000)\n",
      "[1,   298] loss: 0.343766\n",
      "[1,   299] accuracy: (64.000, 100.000)\n",
      "[1,   299] loss: 0.266378\n",
      "[1,   300] accuracy: (36.000, 100.000)\n",
      "[1,   300] loss: 0.335460\n",
      "[1,   301] accuracy: (36.000, 96.000)\n",
      "[1,   301] loss: 0.305406\n",
      "[1,   302] accuracy: (52.000, 96.000)\n",
      "[1,   302] loss: 0.384430\n",
      "[1,   303] accuracy: (48.000, 100.000)\n",
      "[1,   303] loss: 0.376600\n",
      "[1,   304] accuracy: (40.000, 88.000)\n",
      "[1,   304] loss: 0.398445\n",
      "[1,   305] accuracy: (52.000, 100.000)\n",
      "[1,   305] loss: 0.274299\n",
      "[1,   306] accuracy: (36.000, 100.000)\n",
      "[1,   306] loss: 0.264030\n",
      "[1,   307] accuracy: (40.000, 96.000)\n",
      "[1,   307] loss: 0.382786\n",
      "[1,   308] accuracy: (28.000, 100.000)\n",
      "[1,   308] loss: 0.369712\n",
      "[1,   309] accuracy: (36.000, 96.000)\n",
      "[1,   309] loss: 0.351652\n",
      "[1,   310] accuracy: (32.000, 92.000)\n",
      "[1,   310] loss: 0.334933\n",
      "[1,   311] accuracy: (40.000, 100.000)\n",
      "[1,   311] loss: 0.332904\n",
      "[1,   312] accuracy: (40.000, 92.000)\n",
      "[1,   312] loss: 0.355302\n",
      "[1,   313] accuracy: (40.000, 84.000)\n",
      "[1,   313] loss: 0.392579\n",
      "[1,   314] accuracy: (28.000, 92.000)\n",
      "[1,   314] loss: 0.468689\n",
      "[1,   315] accuracy: (52.000, 100.000)\n",
      "[1,   315] loss: 0.427361\n",
      "[1,   316] accuracy: (40.000, 100.000)\n",
      "[1,   316] loss: 0.301387\n",
      "[1,   317] accuracy: (52.000, 100.000)\n",
      "[1,   317] loss: 0.321552\n",
      "[1,   318] accuracy: (44.000, 96.000)\n",
      "[1,   318] loss: 0.303527\n",
      "[1,   319] accuracy: (36.000, 100.000)\n",
      "[1,   319] loss: 0.397089\n",
      "[1,   320] accuracy: (52.000, 92.000)\n",
      "[1,   320] loss: 0.346303\n",
      "[1,   321] accuracy: (44.000, 88.000)\n",
      "[1,   321] loss: 0.346741\n",
      "[1,   322] accuracy: (60.000, 96.000)\n",
      "[1,   322] loss: 0.302123\n",
      "[1,   323] accuracy: (28.000, 100.000)\n",
      "[1,   323] loss: 0.360531\n",
      "[1,   324] accuracy: (32.000, 92.000)\n",
      "[1,   324] loss: 0.330466\n",
      "[1,   325] accuracy: (48.000, 96.000)\n",
      "[1,   325] loss: 0.305865\n",
      "[1,   326] accuracy: (28.000, 96.000)\n",
      "[1,   326] loss: 0.370726\n",
      "[1,   327] accuracy: (40.000, 100.000)\n",
      "[1,   327] loss: 0.319322\n",
      "[1,   328] accuracy: (40.000, 92.000)\n",
      "[1,   328] loss: 0.460406\n",
      "[1,   329] accuracy: (52.000, 100.000)\n",
      "[1,   329] loss: 0.313331\n",
      "[1,   330] accuracy: (48.000, 96.000)\n",
      "[1,   330] loss: 0.312857\n",
      "[1,   331] accuracy: (64.000, 96.000)\n",
      "[1,   331] loss: 0.289523\n",
      "[1,   332] accuracy: (52.000, 96.000)\n",
      "[1,   332] loss: 0.288472\n",
      "[1,   333] accuracy: (44.000, 92.000)\n",
      "[1,   333] loss: 0.344390\n",
      "[1,   334] accuracy: (52.000, 96.000)\n",
      "[1,   334] loss: 0.384675\n",
      "[1,   335] accuracy: (28.000, 88.000)\n",
      "[1,   335] loss: 0.413315\n",
      "[1,   336] accuracy: (36.000, 96.000)\n",
      "[1,   336] loss: 0.316476\n",
      "[1,   337] accuracy: (56.000, 100.000)\n",
      "[1,   337] loss: 0.293501\n",
      "[1,   338] accuracy: (44.000, 84.000)\n",
      "[1,   338] loss: 0.370097\n",
      "[1,   339] accuracy: (44.000, 96.000)\n",
      "[1,   339] loss: 0.386986\n",
      "[1,   340] accuracy: (36.000, 96.000)\n",
      "[1,   340] loss: 0.340638\n",
      "[1,   341] accuracy: (32.000, 92.000)\n",
      "[1,   341] loss: 0.435821\n",
      "[1,   342] accuracy: (44.000, 96.000)\n",
      "[1,   342] loss: 0.286726\n",
      "[1,   343] accuracy: (52.000, 100.000)\n",
      "[1,   343] loss: 0.388378\n",
      "[1,   344] accuracy: (48.000, 100.000)\n",
      "[1,   344] loss: 0.396884\n",
      "[1,   345] accuracy: (44.000, 96.000)\n",
      "[1,   345] loss: 0.364302\n",
      "[1,   346] accuracy: (24.000, 84.000)\n",
      "[1,   346] loss: 0.460506\n",
      "[1,   347] accuracy: (44.000, 96.000)\n",
      "[1,   347] loss: 0.368020\n",
      "[1,   348] accuracy: (32.000, 96.000)\n",
      "[1,   348] loss: 0.335956\n",
      "[1,   349] accuracy: (48.000, 96.000)\n",
      "[1,   349] loss: 0.349944\n",
      "[1,   350] accuracy: (40.000, 96.000)\n",
      "[1,   350] loss: 0.356424\n",
      "[1,   351] accuracy: (56.000, 100.000)\n",
      "[1,   351] loss: 0.288069\n",
      "[1,   352] accuracy: (52.000, 100.000)\n",
      "[1,   352] loss: 0.317357\n",
      "[1,   353] accuracy: (32.000, 100.000)\n",
      "[1,   353] loss: 0.429350\n",
      "[1,   354] accuracy: (52.000, 96.000)\n",
      "[1,   354] loss: 0.324466\n",
      "[1,   355] accuracy: (36.000, 100.000)\n",
      "[1,   355] loss: 0.329526\n",
      "[1,   356] accuracy: (44.000, 96.000)\n",
      "[1,   356] loss: 0.313724\n",
      "[1,   357] accuracy: (56.000, 92.000)\n",
      "[1,   357] loss: 0.299408\n",
      "[1,   358] accuracy: (44.000, 96.000)\n",
      "[1,   358] loss: 0.348174\n",
      "[1,   359] accuracy: (60.000, 92.000)\n",
      "[1,   359] loss: 0.289010\n",
      "[1,   360] accuracy: (44.000, 96.000)\n",
      "[1,   360] loss: 0.329762\n",
      "[1,   361] accuracy: (40.000, 100.000)\n",
      "[1,   361] loss: 0.307852\n",
      "[1,   362] accuracy: (40.000, 96.000)\n",
      "[1,   362] loss: 0.355248\n",
      "[1,   363] accuracy: (28.000, 100.000)\n",
      "[1,   363] loss: 0.318571\n",
      "[1,   364] accuracy: (48.000, 96.000)\n",
      "[1,   364] loss: 0.334082\n",
      "[1,   365] accuracy: (36.000, 88.000)\n",
      "[1,   365] loss: 0.386924\n",
      "[1,   366] accuracy: (36.000, 96.000)\n",
      "[1,   366] loss: 0.296252\n",
      "[1,   367] accuracy: (44.000, 100.000)\n",
      "[1,   367] loss: 0.310055\n",
      "[1,   368] accuracy: (44.000, 96.000)\n",
      "[1,   368] loss: 0.324095\n",
      "[1,   369] accuracy: (24.000, 92.000)\n",
      "[1,   369] loss: 0.394156\n",
      "[1,   370] accuracy: (32.000, 100.000)\n",
      "[1,   370] loss: 0.406502\n",
      "[1,   371] accuracy: (44.000, 100.000)\n",
      "[1,   371] loss: 0.345741\n",
      "[1,   372] accuracy: (36.000, 96.000)\n",
      "[1,   372] loss: 0.360332\n",
      "[1,   373] accuracy: (40.000, 92.000)\n",
      "[1,   373] loss: 0.436555\n",
      "[1,   374] accuracy: (36.000, 100.000)\n",
      "[1,   374] loss: 0.280322\n",
      "[1,   375] accuracy: (48.000, 100.000)\n",
      "[1,   375] loss: 0.339299\n",
      "[1,   376] accuracy: (60.000, 100.000)\n",
      "[1,   376] loss: 0.284879\n",
      "[1,   377] accuracy: (20.000, 96.000)\n",
      "[1,   377] loss: 0.379848\n",
      "[1,   378] accuracy: (36.000, 96.000)\n",
      "[1,   378] loss: 0.267408\n",
      "[1,   379] accuracy: (44.000, 100.000)\n",
      "[1,   379] loss: 0.303316\n",
      "[1,   380] accuracy: (40.000, 96.000)\n",
      "[1,   380] loss: 0.384073\n",
      "[1,   381] accuracy: (32.000, 96.000)\n",
      "[1,   381] loss: 0.425753\n",
      "[1,   382] accuracy: (28.000, 96.000)\n",
      "[1,   382] loss: 0.355519\n",
      "[1,   383] accuracy: (32.000, 96.000)\n",
      "[1,   383] loss: 0.298139\n",
      "[1,   384] accuracy: (40.000, 92.000)\n",
      "[1,   384] loss: 0.330344\n",
      "[1,   385] accuracy: (36.000, 92.000)\n",
      "[1,   385] loss: 0.422010\n",
      "[1,   386] accuracy: (36.000, 100.000)\n",
      "[1,   386] loss: 0.325782\n",
      "[1,   387] accuracy: (48.000, 96.000)\n",
      "[1,   387] loss: 0.317484\n",
      "[1,   388] accuracy: (48.000, 96.000)\n",
      "[1,   388] loss: 0.356512\n",
      "[1,   389] accuracy: (36.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   389] loss: 0.371173\n",
      "[1,   390] accuracy: (36.000, 88.000)\n",
      "[1,   390] loss: 0.377162\n",
      "[1,   391] accuracy: (56.250, 93.750)\n",
      "[1,   391] loss: 0.421205\n",
      "COMPLETED EPOCH 0001... checkpointing here\n",
      "[2,     1] accuracy: (52.000, 100.000)\n",
      "[2,     1] loss: 0.312491\n",
      "[2,     2] accuracy: (40.000, 92.000)\n",
      "[2,     2] loss: 0.307000\n",
      "[2,     3] accuracy: (36.000, 88.000)\n",
      "[2,     3] loss: 0.321937\n",
      "[2,     4] accuracy: (32.000, 100.000)\n",
      "[2,     4] loss: 0.374977\n",
      "[2,     5] accuracy: (36.000, 100.000)\n",
      "[2,     5] loss: 0.251624\n",
      "[2,     6] accuracy: (40.000, 96.000)\n",
      "[2,     6] loss: 0.376945\n",
      "[2,     7] accuracy: (28.000, 92.000)\n",
      "[2,     7] loss: 0.422565\n",
      "[2,     8] accuracy: (32.000, 92.000)\n",
      "[2,     8] loss: 0.352866\n",
      "[2,     9] accuracy: (64.000, 100.000)\n",
      "[2,     9] loss: 0.262220\n",
      "[2,    10] accuracy: (48.000, 96.000)\n",
      "[2,    10] loss: 0.299679\n",
      "[2,    11] accuracy: (56.000, 92.000)\n",
      "[2,    11] loss: 0.273588\n",
      "[2,    12] accuracy: (40.000, 96.000)\n",
      "[2,    12] loss: 0.275242\n",
      "[2,    13] accuracy: (44.000, 96.000)\n",
      "[2,    13] loss: 0.301544\n",
      "[2,    14] accuracy: (64.000, 96.000)\n",
      "[2,    14] loss: 0.335354\n",
      "[2,    15] accuracy: (40.000, 96.000)\n",
      "[2,    15] loss: 0.327547\n",
      "[2,    16] accuracy: (44.000, 96.000)\n",
      "[2,    16] loss: 0.272426\n",
      "[2,    17] accuracy: (60.000, 100.000)\n",
      "[2,    17] loss: 0.308610\n",
      "[2,    18] accuracy: (60.000, 96.000)\n",
      "[2,    18] loss: 0.274283\n",
      "[2,    19] accuracy: (60.000, 100.000)\n",
      "[2,    19] loss: 0.245134\n",
      "[2,    20] accuracy: (56.000, 100.000)\n",
      "[2,    20] loss: 0.285217\n",
      "[2,    21] accuracy: (52.000, 100.000)\n",
      "[2,    21] loss: 0.299062\n",
      "[2,    22] accuracy: (32.000, 92.000)\n",
      "[2,    22] loss: 0.405895\n",
      "[2,    23] accuracy: (44.000, 92.000)\n",
      "[2,    23] loss: 0.301575\n",
      "[2,    24] accuracy: (44.000, 96.000)\n",
      "[2,    24] loss: 0.347307\n",
      "[2,    25] accuracy: (44.000, 100.000)\n",
      "[2,    25] loss: 0.294075\n",
      "[2,    26] accuracy: (36.000, 100.000)\n",
      "[2,    26] loss: 0.351452\n",
      "[2,    27] accuracy: (32.000, 92.000)\n",
      "[2,    27] loss: 0.355195\n",
      "[2,    28] accuracy: (48.000, 100.000)\n",
      "[2,    28] loss: 0.236853\n",
      "[2,    29] accuracy: (32.000, 92.000)\n",
      "[2,    29] loss: 0.369241\n",
      "[2,    30] accuracy: (36.000, 88.000)\n",
      "[2,    30] loss: 0.369841\n",
      "[2,    31] accuracy: (36.000, 100.000)\n",
      "[2,    31] loss: 0.336710\n",
      "[2,    32] accuracy: (44.000, 96.000)\n",
      "[2,    32] loss: 0.313678\n",
      "[2,    33] accuracy: (60.000, 92.000)\n",
      "[2,    33] loss: 0.356714\n",
      "[2,    34] accuracy: (48.000, 96.000)\n",
      "[2,    34] loss: 0.305241\n",
      "[2,    35] accuracy: (24.000, 96.000)\n",
      "[2,    35] loss: 0.329139\n",
      "[2,    36] accuracy: (36.000, 96.000)\n",
      "[2,    36] loss: 0.351974\n",
      "[2,    37] accuracy: (28.000, 100.000)\n",
      "[2,    37] loss: 0.389641\n",
      "[2,    38] accuracy: (32.000, 100.000)\n",
      "[2,    38] loss: 0.328918\n",
      "[2,    39] accuracy: (36.000, 96.000)\n",
      "[2,    39] loss: 0.311147\n",
      "[2,    40] accuracy: (44.000, 96.000)\n",
      "[2,    40] loss: 0.370059\n",
      "[2,    41] accuracy: (44.000, 96.000)\n",
      "[2,    41] loss: 0.287316\n",
      "[2,    42] accuracy: (48.000, 92.000)\n",
      "[2,    42] loss: 0.283355\n",
      "[2,    43] accuracy: (56.000, 100.000)\n",
      "[2,    43] loss: 0.295339\n",
      "[2,    44] accuracy: (56.000, 100.000)\n",
      "[2,    44] loss: 0.286042\n",
      "[2,    45] accuracy: (32.000, 100.000)\n",
      "[2,    45] loss: 0.300141\n",
      "[2,    46] accuracy: (52.000, 96.000)\n",
      "[2,    46] loss: 0.252746\n",
      "[2,    47] accuracy: (52.000, 96.000)\n",
      "[2,    47] loss: 0.271607\n",
      "[2,    48] accuracy: (28.000, 96.000)\n",
      "[2,    48] loss: 0.349033\n",
      "[2,    49] accuracy: (36.000, 100.000)\n",
      "[2,    49] loss: 0.284223\n",
      "[2,    50] accuracy: (36.000, 96.000)\n",
      "[2,    50] loss: 0.350791\n",
      "[2,    51] accuracy: (56.000, 100.000)\n",
      "[2,    51] loss: 0.302353\n",
      "[2,    52] accuracy: (48.000, 100.000)\n",
      "[2,    52] loss: 0.260794\n",
      "[2,    53] accuracy: (48.000, 100.000)\n",
      "[2,    53] loss: 0.340516\n",
      "[2,    54] accuracy: (48.000, 100.000)\n",
      "[2,    54] loss: 0.293836\n",
      "[2,    55] accuracy: (40.000, 96.000)\n",
      "[2,    55] loss: 0.308375\n",
      "[2,    56] accuracy: (44.000, 92.000)\n",
      "[2,    56] loss: 0.387932\n",
      "[2,    57] accuracy: (36.000, 96.000)\n",
      "[2,    57] loss: 0.382969\n",
      "[2,    58] accuracy: (44.000, 92.000)\n",
      "[2,    58] loss: 0.391438\n",
      "[2,    59] accuracy: (56.000, 96.000)\n",
      "[2,    59] loss: 0.308197\n",
      "[2,    60] accuracy: (52.000, 88.000)\n",
      "[2,    60] loss: 0.376619\n",
      "[2,    61] accuracy: (52.000, 96.000)\n",
      "[2,    61] loss: 0.265574\n",
      "[2,    62] accuracy: (44.000, 88.000)\n",
      "[2,    62] loss: 0.397526\n",
      "[2,    63] accuracy: (48.000, 88.000)\n",
      "[2,    63] loss: 0.365737\n",
      "[2,    64] accuracy: (52.000, 100.000)\n",
      "[2,    64] loss: 0.291416\n",
      "[2,    65] accuracy: (36.000, 92.000)\n",
      "[2,    65] loss: 0.378054\n",
      "[2,    66] accuracy: (48.000, 92.000)\n",
      "[2,    66] loss: 0.295210\n",
      "[2,    67] accuracy: (48.000, 100.000)\n",
      "[2,    67] loss: 0.329688\n",
      "[2,    68] accuracy: (40.000, 92.000)\n",
      "[2,    68] loss: 0.382179\n",
      "[2,    69] accuracy: (20.000, 96.000)\n",
      "[2,    69] loss: 0.379865\n",
      "[2,    70] accuracy: (52.000, 96.000)\n",
      "[2,    70] loss: 0.316455\n",
      "[2,    71] accuracy: (36.000, 100.000)\n",
      "[2,    71] loss: 0.249117\n",
      "[2,    72] accuracy: (52.000, 92.000)\n",
      "[2,    72] loss: 0.336355\n",
      "[2,    73] accuracy: (32.000, 96.000)\n",
      "[2,    73] loss: 0.322708\n",
      "[2,    74] accuracy: (20.000, 96.000)\n",
      "[2,    74] loss: 0.319885\n",
      "[2,    75] accuracy: (44.000, 88.000)\n",
      "[2,    75] loss: 0.303067\n",
      "[2,    76] accuracy: (36.000, 92.000)\n",
      "[2,    76] loss: 0.348882\n",
      "[2,    77] accuracy: (48.000, 92.000)\n",
      "[2,    77] loss: 0.318250\n",
      "[2,    78] accuracy: (36.000, 96.000)\n",
      "[2,    78] loss: 0.293455\n",
      "[2,    79] accuracy: (40.000, 96.000)\n",
      "[2,    79] loss: 0.364601\n",
      "[2,    80] accuracy: (60.000, 100.000)\n",
      "[2,    80] loss: 0.297373\n",
      "[2,    81] accuracy: (56.000, 92.000)\n",
      "[2,    81] loss: 0.338137\n",
      "[2,    82] accuracy: (64.000, 100.000)\n",
      "[2,    82] loss: 0.337074\n",
      "[2,    83] accuracy: (36.000, 96.000)\n",
      "[2,    83] loss: 0.386017\n",
      "[2,    84] accuracy: (36.000, 100.000)\n",
      "[2,    84] loss: 0.452295\n",
      "[2,    85] accuracy: (32.000, 96.000)\n",
      "[2,    85] loss: 0.433747\n",
      "[2,    86] accuracy: (48.000, 96.000)\n",
      "[2,    86] loss: 0.365607\n",
      "[2,    87] accuracy: (40.000, 100.000)\n",
      "[2,    87] loss: 0.349351\n",
      "[2,    88] accuracy: (36.000, 92.000)\n",
      "[2,    88] loss: 0.291148\n",
      "[2,    89] accuracy: (32.000, 100.000)\n",
      "[2,    89] loss: 0.388726\n",
      "[2,    90] accuracy: (52.000, 100.000)\n",
      "[2,    90] loss: 0.315147\n",
      "[2,    91] accuracy: (40.000, 100.000)\n",
      "[2,    91] loss: 0.343738\n",
      "[2,    92] accuracy: (44.000, 100.000)\n",
      "[2,    92] loss: 0.350999\n",
      "[2,    93] accuracy: (48.000, 100.000)\n",
      "[2,    93] loss: 0.321593\n",
      "[2,    94] accuracy: (44.000, 92.000)\n",
      "[2,    94] loss: 0.436098\n",
      "[2,    95] accuracy: (56.000, 96.000)\n",
      "[2,    95] loss: 0.399325\n",
      "[2,    96] accuracy: (32.000, 100.000)\n",
      "[2,    96] loss: 0.462579\n",
      "[2,    97] accuracy: (52.000, 100.000)\n",
      "[2,    97] loss: 0.299304\n",
      "[2,    98] accuracy: (40.000, 96.000)\n",
      "[2,    98] loss: 0.314915\n",
      "[2,    99] accuracy: (52.000, 92.000)\n",
      "[2,    99] loss: 0.300285\n",
      "[2,   100] accuracy: (40.000, 100.000)\n",
      "[2,   100] loss: 0.322194\n",
      "[2,   101] accuracy: (44.000, 88.000)\n",
      "[2,   101] loss: 0.354603\n",
      "[2,   102] accuracy: (68.000, 96.000)\n",
      "[2,   102] loss: 0.280138\n",
      "[2,   103] accuracy: (56.000, 92.000)\n",
      "[2,   103] loss: 0.341528\n",
      "[2,   104] accuracy: (40.000, 100.000)\n",
      "[2,   104] loss: 0.275494\n",
      "[2,   105] accuracy: (48.000, 100.000)\n",
      "[2,   105] loss: 0.263727\n",
      "[2,   106] accuracy: (40.000, 100.000)\n",
      "[2,   106] loss: 0.329149\n",
      "[2,   107] accuracy: (36.000, 96.000)\n",
      "[2,   107] loss: 0.394683\n",
      "[2,   108] accuracy: (36.000, 96.000)\n",
      "[2,   108] loss: 0.396197\n",
      "[2,   109] accuracy: (40.000, 96.000)\n",
      "[2,   109] loss: 0.382613\n",
      "[2,   110] accuracy: (36.000, 96.000)\n",
      "[2,   110] loss: 0.355676\n",
      "[2,   111] accuracy: (56.000, 96.000)\n",
      "[2,   111] loss: 0.307686\n",
      "[2,   112] accuracy: (36.000, 96.000)\n",
      "[2,   112] loss: 0.425818\n",
      "[2,   113] accuracy: (48.000, 96.000)\n",
      "[2,   113] loss: 0.301644\n",
      "[2,   114] accuracy: (48.000, 96.000)\n",
      "[2,   114] loss: 0.340270\n",
      "[2,   115] accuracy: (32.000, 92.000)\n",
      "[2,   115] loss: 0.399928\n",
      "[2,   116] accuracy: (48.000, 92.000)\n",
      "[2,   116] loss: 0.385006\n",
      "[2,   117] accuracy: (32.000, 96.000)\n",
      "[2,   117] loss: 0.287832\n",
      "[2,   118] accuracy: (44.000, 96.000)\n",
      "[2,   118] loss: 0.484951\n",
      "[2,   119] accuracy: (44.000, 96.000)\n",
      "[2,   119] loss: 0.278338\n",
      "[2,   120] accuracy: (36.000, 100.000)\n",
      "[2,   120] loss: 0.333191\n",
      "[2,   121] accuracy: (48.000, 100.000)\n",
      "[2,   121] loss: 0.292513\n",
      "[2,   122] accuracy: (36.000, 92.000)\n",
      "[2,   122] loss: 0.319139\n",
      "[2,   123] accuracy: (24.000, 96.000)\n",
      "[2,   123] loss: 0.405815\n",
      "[2,   124] accuracy: (52.000, 100.000)\n",
      "[2,   124] loss: 0.332194\n",
      "[2,   125] accuracy: (24.000, 92.000)\n",
      "[2,   125] loss: 0.338180\n",
      "[2,   126] accuracy: (48.000, 96.000)\n",
      "[2,   126] loss: 0.296878\n",
      "[2,   127] accuracy: (48.000, 92.000)\n",
      "[2,   127] loss: 0.322113\n",
      "[2,   128] accuracy: (56.000, 96.000)\n",
      "[2,   128] loss: 0.225415\n",
      "[2,   129] accuracy: (36.000, 92.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   129] loss: 0.422967\n",
      "[2,   130] accuracy: (36.000, 92.000)\n",
      "[2,   130] loss: 0.387721\n",
      "[2,   131] accuracy: (20.000, 92.000)\n",
      "[2,   131] loss: 0.471947\n",
      "[2,   132] accuracy: (32.000, 96.000)\n",
      "[2,   132] loss: 0.338561\n",
      "[2,   133] accuracy: (56.000, 100.000)\n",
      "[2,   133] loss: 0.303288\n",
      "[2,   134] accuracy: (36.000, 100.000)\n",
      "[2,   134] loss: 0.371570\n",
      "[2,   135] accuracy: (40.000, 92.000)\n",
      "[2,   135] loss: 0.444840\n",
      "[2,   136] accuracy: (48.000, 100.000)\n",
      "[2,   136] loss: 0.336378\n",
      "[2,   137] accuracy: (52.000, 100.000)\n",
      "[2,   137] loss: 0.243283\n",
      "[2,   138] accuracy: (32.000, 96.000)\n",
      "[2,   138] loss: 0.345610\n",
      "[2,   139] accuracy: (24.000, 96.000)\n",
      "[2,   139] loss: 0.368509\n",
      "[2,   140] accuracy: (28.000, 100.000)\n",
      "[2,   140] loss: 0.343215\n",
      "[2,   141] accuracy: (60.000, 100.000)\n",
      "[2,   141] loss: 0.225897\n",
      "[2,   142] accuracy: (40.000, 88.000)\n",
      "[2,   142] loss: 0.339360\n",
      "[2,   143] accuracy: (48.000, 96.000)\n",
      "[2,   143] loss: 0.293561\n",
      "[2,   144] accuracy: (44.000, 100.000)\n",
      "[2,   144] loss: 0.313291\n",
      "[2,   145] accuracy: (44.000, 100.000)\n",
      "[2,   145] loss: 0.344390\n",
      "[2,   146] accuracy: (40.000, 100.000)\n",
      "[2,   146] loss: 0.312503\n",
      "[2,   147] accuracy: (52.000, 100.000)\n",
      "[2,   147] loss: 0.329487\n",
      "[2,   148] accuracy: (40.000, 92.000)\n",
      "[2,   148] loss: 0.375024\n",
      "[2,   149] accuracy: (24.000, 100.000)\n",
      "[2,   149] loss: 0.384372\n",
      "[2,   150] accuracy: (64.000, 100.000)\n",
      "[2,   150] loss: 0.288718\n",
      "[2,   151] accuracy: (40.000, 100.000)\n",
      "[2,   151] loss: 0.332467\n",
      "[2,   152] accuracy: (36.000, 96.000)\n",
      "[2,   152] loss: 0.385817\n",
      "[2,   153] accuracy: (56.000, 100.000)\n",
      "[2,   153] loss: 0.319085\n",
      "[2,   154] accuracy: (56.000, 96.000)\n",
      "[2,   154] loss: 0.280756\n",
      "[2,   155] accuracy: (24.000, 88.000)\n",
      "[2,   155] loss: 0.489219\n",
      "[2,   156] accuracy: (48.000, 96.000)\n",
      "[2,   156] loss: 0.381589\n",
      "[2,   157] accuracy: (52.000, 96.000)\n",
      "[2,   157] loss: 0.359895\n",
      "[2,   158] accuracy: (36.000, 100.000)\n",
      "[2,   158] loss: 0.290423\n",
      "[2,   159] accuracy: (48.000, 88.000)\n",
      "[2,   159] loss: 0.338271\n",
      "[2,   160] accuracy: (32.000, 100.000)\n",
      "[2,   160] loss: 0.321395\n",
      "[2,   161] accuracy: (68.000, 96.000)\n",
      "[2,   161] loss: 0.287623\n",
      "[2,   162] accuracy: (32.000, 88.000)\n",
      "[2,   162] loss: 0.423173\n",
      "[2,   163] accuracy: (36.000, 92.000)\n",
      "[2,   163] loss: 0.358046\n",
      "[2,   164] accuracy: (40.000, 88.000)\n",
      "[2,   164] loss: 0.381894\n",
      "[2,   165] accuracy: (40.000, 100.000)\n",
      "[2,   165] loss: 0.317350\n",
      "[2,   166] accuracy: (40.000, 100.000)\n",
      "[2,   166] loss: 0.353877\n",
      "[2,   167] accuracy: (40.000, 100.000)\n",
      "[2,   167] loss: 0.326350\n",
      "[2,   168] accuracy: (44.000, 96.000)\n",
      "[2,   168] loss: 0.308911\n",
      "[2,   169] accuracy: (36.000, 100.000)\n",
      "[2,   169] loss: 0.397307\n",
      "[2,   170] accuracy: (40.000, 96.000)\n",
      "[2,   170] loss: 0.319764\n",
      "[2,   171] accuracy: (40.000, 100.000)\n",
      "[2,   171] loss: 0.322971\n",
      "[2,   172] accuracy: (36.000, 100.000)\n",
      "[2,   172] loss: 0.357124\n",
      "[2,   173] accuracy: (64.000, 92.000)\n",
      "[2,   173] loss: 0.281657\n",
      "[2,   174] accuracy: (48.000, 92.000)\n",
      "[2,   174] loss: 0.364205\n",
      "[2,   175] accuracy: (36.000, 96.000)\n",
      "[2,   175] loss: 0.388894\n",
      "[2,   176] accuracy: (48.000, 100.000)\n",
      "[2,   176] loss: 0.304532\n",
      "[2,   177] accuracy: (48.000, 96.000)\n",
      "[2,   177] loss: 0.333912\n",
      "[2,   178] accuracy: (44.000, 96.000)\n",
      "[2,   178] loss: 0.337103\n",
      "[2,   179] accuracy: (52.000, 96.000)\n",
      "[2,   179] loss: 0.345582\n",
      "[2,   180] accuracy: (36.000, 96.000)\n",
      "[2,   180] loss: 0.353359\n",
      "[2,   181] accuracy: (44.000, 100.000)\n",
      "[2,   181] loss: 0.346813\n",
      "[2,   182] accuracy: (48.000, 96.000)\n",
      "[2,   182] loss: 0.259024\n",
      "[2,   183] accuracy: (28.000, 92.000)\n",
      "[2,   183] loss: 0.384614\n",
      "[2,   184] accuracy: (48.000, 100.000)\n",
      "[2,   184] loss: 0.316236\n",
      "[2,   185] accuracy: (32.000, 92.000)\n",
      "[2,   185] loss: 0.413876\n",
      "[2,   186] accuracy: (40.000, 96.000)\n",
      "[2,   186] loss: 0.370719\n",
      "[2,   187] accuracy: (32.000, 92.000)\n",
      "[2,   187] loss: 0.416715\n",
      "[2,   188] accuracy: (44.000, 96.000)\n",
      "[2,   188] loss: 0.349149\n",
      "[2,   189] accuracy: (52.000, 88.000)\n",
      "[2,   189] loss: 0.308994\n",
      "[2,   190] accuracy: (24.000, 96.000)\n",
      "[2,   190] loss: 0.334225\n",
      "[2,   191] accuracy: (52.000, 92.000)\n",
      "[2,   191] loss: 0.350993\n",
      "[2,   192] accuracy: (44.000, 100.000)\n",
      "[2,   192] loss: 0.319059\n",
      "[2,   193] accuracy: (48.000, 92.000)\n",
      "[2,   193] loss: 0.361806\n",
      "[2,   194] accuracy: (28.000, 96.000)\n",
      "[2,   194] loss: 0.358864\n",
      "[2,   195] accuracy: (60.000, 92.000)\n",
      "[2,   195] loss: 0.317960\n",
      "[2,   196] accuracy: (56.000, 96.000)\n",
      "[2,   196] loss: 0.308152\n",
      "[2,   197] accuracy: (24.000, 92.000)\n",
      "[2,   197] loss: 0.414950\n",
      "[2,   198] accuracy: (40.000, 96.000)\n",
      "[2,   198] loss: 0.353617\n",
      "[2,   199] accuracy: (52.000, 100.000)\n",
      "[2,   199] loss: 0.377997\n",
      "[2,   200] accuracy: (44.000, 100.000)\n",
      "[2,   200] loss: 0.334697\n",
      "[2,   201] accuracy: (72.000, 100.000)\n",
      "[2,   201] loss: 0.220816\n",
      "[2,   202] accuracy: (40.000, 96.000)\n",
      "[2,   202] loss: 0.345604\n",
      "[2,   203] accuracy: (60.000, 100.000)\n",
      "[2,   203] loss: 0.306216\n",
      "[2,   204] accuracy: (56.000, 96.000)\n",
      "[2,   204] loss: 0.289406\n",
      "[2,   205] accuracy: (52.000, 100.000)\n",
      "[2,   205] loss: 0.297112\n",
      "[2,   206] accuracy: (72.000, 92.000)\n",
      "[2,   206] loss: 0.257874\n",
      "[2,   207] accuracy: (40.000, 100.000)\n",
      "[2,   207] loss: 0.280301\n",
      "[2,   208] accuracy: (52.000, 96.000)\n",
      "[2,   208] loss: 0.309834\n",
      "[2,   209] accuracy: (60.000, 100.000)\n",
      "[2,   209] loss: 0.351390\n",
      "[2,   210] accuracy: (40.000, 100.000)\n",
      "[2,   210] loss: 0.392238\n",
      "[2,   211] accuracy: (40.000, 96.000)\n",
      "[2,   211] loss: 0.317110\n",
      "[2,   212] accuracy: (44.000, 96.000)\n",
      "[2,   212] loss: 0.284025\n",
      "[2,   213] accuracy: (32.000, 100.000)\n",
      "[2,   213] loss: 0.299210\n",
      "[2,   214] accuracy: (40.000, 100.000)\n",
      "[2,   214] loss: 0.321444\n",
      "[2,   215] accuracy: (48.000, 96.000)\n",
      "[2,   215] loss: 0.398609\n",
      "[2,   216] accuracy: (44.000, 88.000)\n",
      "[2,   216] loss: 0.312328\n",
      "[2,   217] accuracy: (44.000, 96.000)\n",
      "[2,   217] loss: 0.252969\n",
      "[2,   218] accuracy: (44.000, 92.000)\n",
      "[2,   218] loss: 0.373439\n",
      "[2,   219] accuracy: (64.000, 96.000)\n",
      "[2,   219] loss: 0.287201\n",
      "[2,   220] accuracy: (60.000, 92.000)\n",
      "[2,   220] loss: 0.366700\n",
      "[2,   221] accuracy: (52.000, 96.000)\n",
      "[2,   221] loss: 0.307452\n",
      "[2,   222] accuracy: (48.000, 96.000)\n",
      "[2,   222] loss: 0.374935\n",
      "[2,   223] accuracy: (52.000, 96.000)\n",
      "[2,   223] loss: 0.381764\n",
      "[2,   224] accuracy: (40.000, 100.000)\n",
      "[2,   224] loss: 0.345169\n",
      "[2,   225] accuracy: (56.000, 88.000)\n",
      "[2,   225] loss: 0.347565\n",
      "[2,   226] accuracy: (40.000, 96.000)\n",
      "[2,   226] loss: 0.378652\n",
      "[2,   227] accuracy: (36.000, 96.000)\n",
      "[2,   227] loss: 0.336767\n",
      "[2,   228] accuracy: (36.000, 100.000)\n",
      "[2,   228] loss: 0.303054\n",
      "[2,   229] accuracy: (44.000, 88.000)\n",
      "[2,   229] loss: 0.371341\n",
      "[2,   230] accuracy: (36.000, 96.000)\n",
      "[2,   230] loss: 0.364302\n",
      "[2,   231] accuracy: (48.000, 92.000)\n",
      "[2,   231] loss: 0.276353\n",
      "[2,   232] accuracy: (48.000, 100.000)\n",
      "[2,   232] loss: 0.383298\n",
      "[2,   233] accuracy: (28.000, 84.000)\n",
      "[2,   233] loss: 0.334092\n",
      "[2,   234] accuracy: (48.000, 96.000)\n",
      "[2,   234] loss: 0.250805\n",
      "[2,   235] accuracy: (44.000, 88.000)\n",
      "[2,   235] loss: 0.352666\n",
      "[2,   236] accuracy: (64.000, 96.000)\n",
      "[2,   236] loss: 0.406439\n",
      "[2,   237] accuracy: (24.000, 96.000)\n",
      "[2,   237] loss: 0.357060\n",
      "[2,   238] accuracy: (48.000, 100.000)\n",
      "[2,   238] loss: 0.279279\n",
      "[2,   239] accuracy: (44.000, 100.000)\n",
      "[2,   239] loss: 0.332543\n",
      "[2,   240] accuracy: (36.000, 100.000)\n",
      "[2,   240] loss: 0.288908\n",
      "[2,   241] accuracy: (40.000, 100.000)\n",
      "[2,   241] loss: 0.395825\n",
      "[2,   242] accuracy: (48.000, 100.000)\n",
      "[2,   242] loss: 0.328960\n",
      "[2,   243] accuracy: (44.000, 92.000)\n",
      "[2,   243] loss: 0.329896\n",
      "[2,   244] accuracy: (40.000, 100.000)\n",
      "[2,   244] loss: 0.386723\n",
      "[2,   245] accuracy: (40.000, 92.000)\n",
      "[2,   245] loss: 0.341110\n",
      "[2,   246] accuracy: (56.000, 100.000)\n",
      "[2,   246] loss: 0.288114\n",
      "[2,   247] accuracy: (60.000, 100.000)\n",
      "[2,   247] loss: 0.327430\n",
      "[2,   248] accuracy: (44.000, 92.000)\n",
      "[2,   248] loss: 0.405214\n",
      "[2,   249] accuracy: (52.000, 92.000)\n",
      "[2,   249] loss: 0.361732\n",
      "[2,   250] accuracy: (56.000, 96.000)\n",
      "[2,   250] loss: 0.252282\n",
      "[2,   251] accuracy: (24.000, 96.000)\n",
      "[2,   251] loss: 0.340070\n",
      "[2,   252] accuracy: (48.000, 92.000)\n",
      "[2,   252] loss: 0.250866\n",
      "[2,   253] accuracy: (48.000, 92.000)\n",
      "[2,   253] loss: 0.316305\n",
      "[2,   254] accuracy: (56.000, 100.000)\n",
      "[2,   254] loss: 0.323161\n",
      "[2,   255] accuracy: (64.000, 100.000)\n",
      "[2,   255] loss: 0.209985\n",
      "[2,   256] accuracy: (40.000, 100.000)\n",
      "[2,   256] loss: 0.343203\n",
      "[2,   257] accuracy: (60.000, 92.000)\n",
      "[2,   257] loss: 0.303497\n",
      "[2,   258] accuracy: (40.000, 100.000)\n",
      "[2,   258] loss: 0.271102\n",
      "[2,   259] accuracy: (28.000, 96.000)\n",
      "[2,   259] loss: 0.415323\n",
      "[2,   260] accuracy: (36.000, 100.000)\n",
      "[2,   260] loss: 0.396170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   261] accuracy: (52.000, 96.000)\n",
      "[2,   261] loss: 0.303209\n",
      "[2,   262] accuracy: (56.000, 88.000)\n",
      "[2,   262] loss: 0.298842\n",
      "[2,   263] accuracy: (36.000, 92.000)\n",
      "[2,   263] loss: 0.358751\n",
      "[2,   264] accuracy: (60.000, 100.000)\n",
      "[2,   264] loss: 0.225620\n",
      "[2,   265] accuracy: (32.000, 92.000)\n",
      "[2,   265] loss: 0.339437\n",
      "[2,   266] accuracy: (28.000, 92.000)\n",
      "[2,   266] loss: 0.339971\n",
      "[2,   267] accuracy: (52.000, 96.000)\n",
      "[2,   267] loss: 0.290660\n",
      "[2,   268] accuracy: (40.000, 92.000)\n",
      "[2,   268] loss: 0.347966\n",
      "[2,   269] accuracy: (40.000, 96.000)\n",
      "[2,   269] loss: 0.343394\n",
      "[2,   270] accuracy: (24.000, 88.000)\n",
      "[2,   270] loss: 0.370747\n",
      "[2,   271] accuracy: (44.000, 92.000)\n",
      "[2,   271] loss: 0.442298\n",
      "[2,   272] accuracy: (28.000, 96.000)\n",
      "[2,   272] loss: 0.355603\n",
      "[2,   273] accuracy: (48.000, 92.000)\n",
      "[2,   273] loss: 0.338737\n",
      "[2,   274] accuracy: (36.000, 92.000)\n",
      "[2,   274] loss: 0.362684\n",
      "[2,   275] accuracy: (48.000, 92.000)\n",
      "[2,   275] loss: 0.348504\n",
      "[2,   276] accuracy: (36.000, 96.000)\n",
      "[2,   276] loss: 0.415625\n",
      "[2,   277] accuracy: (36.000, 100.000)\n",
      "[2,   277] loss: 0.315391\n",
      "[2,   278] accuracy: (28.000, 100.000)\n",
      "[2,   278] loss: 0.397350\n",
      "[2,   279] accuracy: (56.000, 96.000)\n",
      "[2,   279] loss: 0.263384\n",
      "[2,   280] accuracy: (52.000, 88.000)\n",
      "[2,   280] loss: 0.382301\n",
      "[2,   281] accuracy: (48.000, 92.000)\n",
      "[2,   281] loss: 0.290650\n",
      "[2,   282] accuracy: (52.000, 92.000)\n",
      "[2,   282] loss: 0.321887\n",
      "[2,   283] accuracy: (40.000, 100.000)\n",
      "[2,   283] loss: 0.388700\n",
      "[2,   284] accuracy: (36.000, 96.000)\n",
      "[2,   284] loss: 0.308800\n",
      "[2,   285] accuracy: (36.000, 96.000)\n",
      "[2,   285] loss: 0.403358\n",
      "[2,   286] accuracy: (36.000, 96.000)\n",
      "[2,   286] loss: 0.489446\n",
      "[2,   287] accuracy: (40.000, 96.000)\n",
      "[2,   287] loss: 0.395014\n",
      "[2,   288] accuracy: (36.000, 92.000)\n",
      "[2,   288] loss: 0.340661\n",
      "[2,   289] accuracy: (44.000, 100.000)\n",
      "[2,   289] loss: 0.309665\n",
      "[2,   290] accuracy: (60.000, 100.000)\n",
      "[2,   290] loss: 0.289288\n",
      "[2,   291] accuracy: (40.000, 92.000)\n",
      "[2,   291] loss: 0.315979\n",
      "[2,   292] accuracy: (52.000, 96.000)\n",
      "[2,   292] loss: 0.367593\n",
      "[2,   293] accuracy: (44.000, 96.000)\n",
      "[2,   293] loss: 0.325765\n",
      "[2,   294] accuracy: (40.000, 88.000)\n",
      "[2,   294] loss: 0.360965\n",
      "[2,   295] accuracy: (40.000, 96.000)\n",
      "[2,   295] loss: 0.344434\n",
      "[2,   296] accuracy: (44.000, 88.000)\n",
      "[2,   296] loss: 0.383156\n",
      "[2,   297] accuracy: (48.000, 100.000)\n",
      "[2,   297] loss: 0.301408\n",
      "[2,   298] accuracy: (52.000, 100.000)\n",
      "[2,   298] loss: 0.317132\n",
      "[2,   299] accuracy: (52.000, 100.000)\n",
      "[2,   299] loss: 0.338096\n",
      "[2,   300] accuracy: (48.000, 100.000)\n",
      "[2,   300] loss: 0.368874\n",
      "[2,   301] accuracy: (60.000, 92.000)\n",
      "[2,   301] loss: 0.271995\n",
      "[2,   302] accuracy: (36.000, 96.000)\n",
      "[2,   302] loss: 0.361160\n",
      "[2,   303] accuracy: (28.000, 88.000)\n",
      "[2,   303] loss: 0.370644\n",
      "[2,   304] accuracy: (64.000, 88.000)\n",
      "[2,   304] loss: 0.332167\n",
      "[2,   305] accuracy: (44.000, 96.000)\n",
      "[2,   305] loss: 0.363697\n",
      "[2,   306] accuracy: (36.000, 80.000)\n",
      "[2,   306] loss: 0.404337\n",
      "[2,   307] accuracy: (44.000, 96.000)\n",
      "[2,   307] loss: 0.407920\n",
      "[2,   308] accuracy: (36.000, 92.000)\n",
      "[2,   308] loss: 0.415085\n",
      "[2,   309] accuracy: (36.000, 96.000)\n",
      "[2,   309] loss: 0.389712\n",
      "[2,   310] accuracy: (60.000, 100.000)\n",
      "[2,   310] loss: 0.348502\n",
      "[2,   311] accuracy: (28.000, 96.000)\n",
      "[2,   311] loss: 0.365767\n",
      "[2,   312] accuracy: (56.000, 92.000)\n",
      "[2,   312] loss: 0.384243\n",
      "[2,   313] accuracy: (44.000, 96.000)\n",
      "[2,   313] loss: 0.350719\n",
      "[2,   314] accuracy: (48.000, 92.000)\n",
      "[2,   314] loss: 0.414830\n",
      "[2,   315] accuracy: (40.000, 96.000)\n",
      "[2,   315] loss: 0.354958\n",
      "[2,   316] accuracy: (60.000, 96.000)\n",
      "[2,   316] loss: 0.253047\n",
      "[2,   317] accuracy: (52.000, 96.000)\n",
      "[2,   317] loss: 0.299154\n",
      "[2,   318] accuracy: (52.000, 100.000)\n",
      "[2,   318] loss: 0.338736\n",
      "[2,   319] accuracy: (40.000, 96.000)\n",
      "[2,   319] loss: 0.389202\n",
      "[2,   320] accuracy: (60.000, 92.000)\n",
      "[2,   320] loss: 0.368744\n",
      "[2,   321] accuracy: (40.000, 92.000)\n",
      "[2,   321] loss: 0.406275\n",
      "[2,   322] accuracy: (48.000, 92.000)\n",
      "[2,   322] loss: 0.385182\n",
      "[2,   323] accuracy: (60.000, 96.000)\n",
      "[2,   323] loss: 0.272431\n",
      "[2,   324] accuracy: (40.000, 88.000)\n",
      "[2,   324] loss: 0.352984\n",
      "[2,   325] accuracy: (44.000, 100.000)\n",
      "[2,   325] loss: 0.265666\n",
      "[2,   326] accuracy: (60.000, 100.000)\n",
      "[2,   326] loss: 0.273011\n",
      "[2,   327] accuracy: (36.000, 88.000)\n",
      "[2,   327] loss: 0.425580\n",
      "[2,   328] accuracy: (44.000, 100.000)\n",
      "[2,   328] loss: 0.328737\n",
      "[2,   329] accuracy: (44.000, 96.000)\n",
      "[2,   329] loss: 0.335878\n",
      "[2,   330] accuracy: (48.000, 84.000)\n",
      "[2,   330] loss: 0.344819\n",
      "[2,   331] accuracy: (40.000, 92.000)\n",
      "[2,   331] loss: 0.394798\n",
      "[2,   332] accuracy: (52.000, 100.000)\n",
      "[2,   332] loss: 0.270425\n",
      "[2,   333] accuracy: (36.000, 100.000)\n",
      "[2,   333] loss: 0.296547\n",
      "[2,   334] accuracy: (40.000, 100.000)\n",
      "[2,   334] loss: 0.300232\n",
      "[2,   335] accuracy: (44.000, 100.000)\n",
      "[2,   335] loss: 0.244565\n",
      "[2,   336] accuracy: (52.000, 100.000)\n",
      "[2,   336] loss: 0.273250\n",
      "[2,   337] accuracy: (52.000, 96.000)\n",
      "[2,   337] loss: 0.382652\n",
      "[2,   338] accuracy: (24.000, 92.000)\n",
      "[2,   338] loss: 0.320229\n",
      "[2,   339] accuracy: (36.000, 96.000)\n",
      "[2,   339] loss: 0.422533\n",
      "[2,   340] accuracy: (52.000, 92.000)\n",
      "[2,   340] loss: 0.379669\n",
      "[2,   341] accuracy: (44.000, 96.000)\n",
      "[2,   341] loss: 0.369306\n",
      "[2,   342] accuracy: (52.000, 100.000)\n",
      "[2,   342] loss: 0.254553\n",
      "[2,   343] accuracy: (52.000, 100.000)\n",
      "[2,   343] loss: 0.297796\n",
      "[2,   344] accuracy: (52.000, 96.000)\n",
      "[2,   344] loss: 0.326848\n",
      "[2,   345] accuracy: (36.000, 96.000)\n",
      "[2,   345] loss: 0.325492\n",
      "[2,   346] accuracy: (60.000, 100.000)\n",
      "[2,   346] loss: 0.304860\n",
      "[2,   347] accuracy: (48.000, 96.000)\n",
      "[2,   347] loss: 0.284041\n",
      "[2,   348] accuracy: (56.000, 88.000)\n",
      "[2,   348] loss: 0.342460\n",
      "[2,   349] accuracy: (24.000, 92.000)\n",
      "[2,   349] loss: 0.389835\n",
      "[2,   350] accuracy: (44.000, 96.000)\n",
      "[2,   350] loss: 0.325858\n",
      "[2,   351] accuracy: (48.000, 96.000)\n",
      "[2,   351] loss: 0.292751\n",
      "[2,   352] accuracy: (52.000, 100.000)\n",
      "[2,   352] loss: 0.288025\n",
      "[2,   353] accuracy: (48.000, 92.000)\n",
      "[2,   353] loss: 0.322678\n",
      "[2,   354] accuracy: (44.000, 96.000)\n",
      "[2,   354] loss: 0.312853\n",
      "[2,   355] accuracy: (40.000, 96.000)\n",
      "[2,   355] loss: 0.465550\n",
      "[2,   356] accuracy: (52.000, 96.000)\n",
      "[2,   356] loss: 0.304397\n",
      "[2,   357] accuracy: (44.000, 100.000)\n",
      "[2,   357] loss: 0.329970\n",
      "[2,   358] accuracy: (48.000, 96.000)\n",
      "[2,   358] loss: 0.338922\n",
      "[2,   359] accuracy: (40.000, 96.000)\n",
      "[2,   359] loss: 0.388468\n",
      "[2,   360] accuracy: (32.000, 92.000)\n",
      "[2,   360] loss: 0.345132\n",
      "[2,   361] accuracy: (36.000, 100.000)\n",
      "[2,   361] loss: 0.434448\n",
      "[2,   362] accuracy: (48.000, 92.000)\n",
      "[2,   362] loss: 0.357725\n",
      "[2,   363] accuracy: (52.000, 88.000)\n",
      "[2,   363] loss: 0.348761\n",
      "[2,   364] accuracy: (48.000, 100.000)\n",
      "[2,   364] loss: 0.292080\n",
      "[2,   365] accuracy: (60.000, 96.000)\n",
      "[2,   365] loss: 0.354268\n",
      "[2,   366] accuracy: (36.000, 100.000)\n",
      "[2,   366] loss: 0.311754\n",
      "[2,   367] accuracy: (24.000, 100.000)\n",
      "[2,   367] loss: 0.284195\n",
      "[2,   368] accuracy: (40.000, 92.000)\n",
      "[2,   368] loss: 0.384058\n",
      "[2,   369] accuracy: (44.000, 100.000)\n",
      "[2,   369] loss: 0.368963\n",
      "[2,   370] accuracy: (48.000, 100.000)\n",
      "[2,   370] loss: 0.256852\n",
      "[2,   371] accuracy: (32.000, 96.000)\n",
      "[2,   371] loss: 0.388427\n",
      "[2,   372] accuracy: (36.000, 92.000)\n",
      "[2,   372] loss: 0.442352\n",
      "[2,   373] accuracy: (44.000, 100.000)\n",
      "[2,   373] loss: 0.355418\n",
      "[2,   374] accuracy: (48.000, 100.000)\n",
      "[2,   374] loss: 0.325616\n",
      "[2,   375] accuracy: (44.000, 96.000)\n",
      "[2,   375] loss: 0.340886\n",
      "[2,   376] accuracy: (48.000, 96.000)\n",
      "[2,   376] loss: 0.302533\n",
      "[2,   377] accuracy: (52.000, 96.000)\n",
      "[2,   377] loss: 0.268988\n",
      "[2,   378] accuracy: (48.000, 100.000)\n",
      "[2,   378] loss: 0.291454\n",
      "[2,   379] accuracy: (52.000, 100.000)\n",
      "[2,   379] loss: 0.314955\n",
      "[2,   380] accuracy: (32.000, 100.000)\n",
      "[2,   380] loss: 0.291282\n",
      "[2,   381] accuracy: (68.000, 100.000)\n",
      "[2,   381] loss: 0.297668\n",
      "[2,   382] accuracy: (48.000, 96.000)\n",
      "[2,   382] loss: 0.351016\n",
      "[2,   383] accuracy: (60.000, 96.000)\n",
      "[2,   383] loss: 0.364937\n",
      "[2,   384] accuracy: (52.000, 88.000)\n",
      "[2,   384] loss: 0.265745\n",
      "[2,   385] accuracy: (48.000, 96.000)\n",
      "[2,   385] loss: 0.294739\n",
      "[2,   386] accuracy: (44.000, 96.000)\n",
      "[2,   386] loss: 0.324599\n",
      "[2,   387] accuracy: (52.000, 100.000)\n",
      "[2,   387] loss: 0.265127\n",
      "[2,   388] accuracy: (60.000, 96.000)\n",
      "[2,   388] loss: 0.223584\n",
      "[2,   389] accuracy: (44.000, 96.000)\n",
      "[2,   389] loss: 0.382790\n",
      "[2,   390] accuracy: (44.000, 96.000)\n",
      "[2,   390] loss: 0.307087\n",
      "[2,   391] accuracy: (50.000, 93.750)\n",
      "[2,   391] loss: 0.279676\n",
      "COMPLETED EPOCH 0002... checkpointing here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss = nn.CrossEntropyLoss() # just use standard XEntropy to train\n",
    "training_logger = training_obj.train(cifar_trainset, 2, train_loss, \n",
    "                                     attack_parameters=attack_params, \n",
    "                                     verbosity='snoop', loglevel='snoop') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printouts look like:\n",
    "``` \n",
    "[epoch_no, minibatch_no] accuracy: (X, Y) \n",
    "[epoch_no, minibatch_no] loss: Z\n",
    "```\n",
    "\n",
    "- X is the percent of successfully classified *adversarial* examples generated from that minibatch only\n",
    "- Y is the percent of successfully classified *original* examples on that minibatch only\n",
    "- Z is the value of the loss function after that minibatch\n",
    "\n",
    "The output of training is a `TrainingLogger` class that stores essentially what was printed, and is separately controlled by its own `loglevel` argument. This can be accessed (and just to be safe, sorted using `sort_series`) and plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c001e5470>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c0359b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(training_logger.sort_series('training_loss', return_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c27654e48>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c27504e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot adversarial accuracy as we train \n",
    "# This should be noisy, but generally going UP\n",
    "plt.plot([_[0] for _ in training_logger.sort_series('attack', return_keys=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training completes, you can verify that the checkpoints are indeed stored in wherever you have set up pretrained models to be stored. By default this is `mister_ed/pretrained_models/`, so you should have a `tutorial_fgsm.resnet20.000002.path.tar` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restarting from Checkpoint\n",
    "When training, sometimes @#\\$& happens and things break. This is why we checkpoint. Here we'll show how to restart from checkpoint in training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to pick back up from where we left off with the experiment/architecture pair defined above `(tutorial_fgsm, resnet20)`. Then we want to do the following steps:\n",
    "\n",
    "1. Instantiate a model of the same architecture (weights don't matter, since we'll load from the checkpoint) \n",
    "2. Build an `AdversarialTraining` object using this model, its normalizer, and the same experiment name, architecture name \n",
    "3. Build a loss function, attack_parameters object, and all other identical kwargs from the first (aborted) training run \n",
    "4. Run the training using the training object's `train_from_checkpoint` method instead of `train`. All the kwargs are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,   100] accuracy: (40.000, 100.000)\n",
      "[3,   100] loss: 0.335839\n",
      "[3,   200] accuracy: (36.000, 92.000)\n",
      "[3,   200] loss: 0.343946\n",
      "[3,   300] accuracy: (40.000, 92.000)\n",
      "[3,   300] loss: 0.334347\n",
      "COMPLETED EPOCH 0003... checkpointing here\n",
      "[4,   100] accuracy: (36.000, 92.000)\n",
      "[4,   100] loss: 0.322878\n",
      "[4,   200] accuracy: (36.000, 100.000)\n",
      "[4,   200] loss: 0.317655\n",
      "[4,   300] accuracy: (60.000, 100.000)\n",
      "[4,   300] loss: 0.331455\n",
      "COMPLETED EPOCH 0004... checkpointing here\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "naive_model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)\n",
    "new_train_obj = advtrain.AdversarialTraining(naive_model, normalizer, experiment_name, architecture)\n",
    "\n",
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(naive_model, normalizer)\n",
    "attack_object = aa.FGSM(naive_model, normalizer, delta_threat, attack_loss)\n",
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n",
    "\n",
    "new_train_obj.train_from_checkpoint(cifar_trainset, 4, train_loss, attack_parameters=attack_params, \n",
    "                                    verbosity='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this finishes, notice that you should now have a file \n",
    "`tutorial_fgsm.resnet20.000004.path.tar` in your pretrained_models directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the training script \n",
    "Using an ipython notebook isn't typically ideal for training, since it mandates you keep your browser window open. To this end, we've built a script to perform adversarial training in a tmux/screen background. This is located in `scripts/advtrain.py`. Here's what we've found are best practices for doing this:\n",
    "\n",
    "- Copy `scripts/advtrain.py` into `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py`\n",
    "- Modify the `build_attack_params` method in `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py` to use the attack parameters that you want. There's plenty of prebuilt attack parameters in that file to choose from. \n",
    "- In a tmux/screen, from `mister_ed`, run \n",
    "\n",
    "```python -m scripts.advtrain_DESCRIPTIVE_EXPERIMENT_NAME --exp <DESCRIPTIVE_EXPERIMENT_NAME> --arch <ARCHITECTURE_CHOICE> --verbosity [snoop/high/medium]```\n",
    "\n",
    "- To resume, you can optionally add the `-r` or `--resume` flag to the script call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on GPU Usage\n",
    "If you have access to a GPU on your machine, you'll probably want to leverage its power when doing training and attacks. `mister_ed` has been designed so \"standard\" GPU behavior should be supported without any extra effort. By \"standard\" GPU behavior, I mean that all objects reside on the same device: either all on the GPU or none on the GPU. If there is a GPU on your machine, which one can check from the output of \n",
    "```\n",
    "import torch.cuda as cuda \n",
    "print(cuda.is_available()) \n",
    "```\n",
    "Globally, unless otherwise specified, all objects will be initialized in GPU-mode if this output is `True`. This is done behind-the-scenes by setting the environment variable `MISTER_ED_GPU`. If you have access to a GPU, but wouldn't like to use it, you can manually override this environment variable by calling:\n",
    "```\n",
    "import utils.pytorch_utils as utils \n",
    "utils.set_global_gpu(False)\n",
    "```\n",
    "And then none of your objects will be in GPU-mode by default. \n",
    "\n",
    "For nonstandard GPU behavior, you should initialize any object that differs from the default gpu status (as defined by `MISTER_ED_GPU`) with the kwarg `manual_gpu=<True/False>`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
